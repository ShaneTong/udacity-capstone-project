{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Data Preprocessing\n",
    "\n",
    "_Author: Yifei Tong_\n",
    "\n",
    "---\n",
    "## Goal\n",
    "\n",
    "The goal of this notebook is to preprocess the previously collected financial news dataset. The main steps include removing stopwords, stemming words, vectorizing news articles, and etc. \n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Read collected data from a pickle file in the S3 bucket\n",
    "2. Preprocess data\n",
    "    - Split dataset into a training dataset and a test dataset\n",
    "    - Turn characters to lower case\n",
    "    - Remove punctuations\n",
    "    - Remove stopwords\n",
    "    - Stemmed all words\n",
    "    - Create a word dictionary\n",
    "    - Vectorize texts\n",
    "3. Save data in a file in proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-2-989457217313\n"
     ]
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket_name = session.default_bucket()\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '../data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "dataset_prefix = 'financial-news-dataset'\n",
    "pkl_file_name = 'news_dataset.pickle'\n",
    "pkl_file_key = os.path.join(dataset_prefix, pkl_file_name)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "pickle_file = s3_client.get_object(Bucket=bucket_name, Key=os.path.join(dataset_prefix, pkl_file_name))['Body'].read()\n",
    "dataset = pickle.loads(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_X = [art['text'] for art in dataset['google']]\n",
    "google_y = [art['percentage_change']*100 for art in dataset['google']]\n",
    "amazon_X = [art['text'] for art in dataset['amazon']]\n",
    "amazon_y = [art['percentage_change']*100 for art in dataset['amazon']]\n",
    "facebook_X = [art['text'] for art in dataset['facebook']]\n",
    "facebook_y = [art['percentage_change']*100 for art in dataset['facebook']]\n",
    "microsoft_X = [art['text'] for art in dataset['microsoft']]\n",
    "microsoft_y = [art['percentage_change']*100 for art in dataset['microsoft']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u.s. news texas serial bomber made video confession before blowing himself up: police the serial bomber whose deadly attacks terrorized austin, texas, for weeks left a 25-minute video \"confession\" on a cell phone. the suspect blew himself up on wednesday as officers closed in to make an arrest. the video failed to reveal a coherent motive for the attacks. published 6 hours ago reuters source: sinclair broadcast group mark anthony conditt, suspect in the austin bombings scene on security video at a fedex facility. \\nthe serial bomber whose deadly attacks terrorized austin, texas, for weeks left a 25-minute video \"confession\" on a cell phone found after he blew himself up on wednesday as officers closed in to make an arrest, police said. \\nmark conditt, 23, an unemployed man from the suburb of pflugerville, detailed how he made all seven bombs that have been accounted for — five that exploded, one that was recovered before it went off and a seventh that he detonated as officers rushed his vehicle early on wednesday. \\nbut the video failed to reveal a coherent motive for the attacks spread over the past three weeks, police said. \\n\"he does not at all mention anything about terrorism, nor does he mention anything about hate, but instead it is the outcry of a very challenged young man, talking about challenges in his personal life,\" austin police chief brian manley told reporters. \\n\"i would classify this as a confession,\" manley said. \\nconditt, who had never before been in trouble with the law, killed two people and wounded five with a campaign of violence that began on march 2, authorities said. \\nbased on their search of the suspect\\'s home and his video statement, authorities said they felt confident that there were no other bombs and that the public was safe from further harm. \\nfbi special agent christopher combs said investigators believe the suspect would have continued his attacks had he not been apprehended. \\npolice recovered a \"target list\" of addresses for future bombings, the los angeles times reported, citing u.s. representative michael mccaul of texas, the republican chairman of the house homeland security committee. \\neven so, the video gave no explanation for the individuals and addresses singled out as recipients of the bombs that were planted or shipped, manley said. \\npolice previously said they had considered the possibility that the attacks were racially motivated, noting that the first several victims, including the two who died, were either african-american or hispanic. \\nconditt likely recorded the video between 9 p.m. and 11 p.m. on tuesday. according to manley, conditt said he believed police \"were getting very close to him,\" and he was right. authorities filed a criminal complaint and issued an arrest warrant around that time. \\nby wednesday morning, police had tracked conditt to a hotel and were waiting for the arrival of tactical units and equipment before they planned to make an arrest, manley said. but then conditt drove away. \\npolice followed and decided to stop him before he got on the highway. just as officers approached the vehicle, the explosion went off, manley said. there was also some police shooting. \\n\"this can never be called a happy ending, but it\\'s a damn good one for the people of this community, the people of the state of texas,\" travis county district attorney margaret moore told reporters. \\nresidents in austin, a city of 1 million people and a liberal enclave of university students and tech companies, voiced relief that the hunt for the serial bomber was over. \\n\"i am going to be leery and extra careful tomorrow at work, but i feel relieved now,\" said jesus borjon, 44, an employee of parcel delivery firm ups, who lives in pflugerville. \\naustin was hosting thousands of out-of-town visitors for its annual south by southwest festival of music, film and technology when the first bombings occurred. trail of clues \\nthe trail of clues leading hundreds of investigators to the serial bomber ranged from store receipts and fragments of booby-trapped packages to surveillance video of the suspect in a hat and wig. \\nexperts scoured the suspect\\'s home for further evidence on wednesday, removing explosive materials and bomb components. \\n\"i wouldn\\'t call it a bomb-making factory, but there\\'s definitely components consistent with what we\\'ve seen in all these other devices,\" fred milanowski, special agent in charge of houston office of the bureau of alcohol, tobacco, firearms and explosives, told reporters. \\ninvestigators evacuated a four-block radius around conditt\\'s house while they searched the home, which conditt shared with two roommates who had been detained for questioning. conditt moved in a year ago after leaving his parents\\' home about a mile (1.6 km) away, public records showed. \\none law enforcement official involved in the investigation but speaking on condition of anonymity told reuters that some of the materials found in remnants of the bombs were traced back to where they had been sold. \\nthe source also said investigators, once they had identified conditt as a potential suspect, obtained a warrant to monitor his google search history. \\nsurveillance video showed the suspect in a hat and a blond wig, as he prepared to ship one of two booby-trapped packages he was known to have sent through fedex corp\\'s delivery service, according to the source. \\nhe used the alias \"kelly killmore\" to ship those packages, abc news reported, citing unnamed law enforcement sources. \\nconditt, who was home-schooled, described himself as a conservative but said he was not politically inclined, according to blog posts he wrote as part of a u.s. politics class at austin community college. he attended from 2010 to 2012 and had no record of any disciplinary actions, the school said.',\n",
       " 'a slight gain on friday was not enough to stop stocks from posting a loss this week, weighed down by fears of a possible trade war and white house turmoil.\\nthe s&p 500 notched a 1.2 percent loss for the week, despite a 0.2 percent gain on friday. the dow jones industrial average also fell 1.5 percent on the week as shares of boeing dropped 6.8 percent on the trade tensions. the dow closed 72.85 points higher on friday at 24,946.51.\\nthe nasdaq composite closed flat at 7,481.99 amid a 1.4 percent decline in google-parent alphabet and a 0.7 percent fall in amazon shares. the index also fell 1 percent for the week.\\ntariffs on steel and aluminum imports are expected to come into effect in the coming weeks, after trump signed two declarations last week. while canada and mexico are exempt from the deal, investors worry that countries around the world including china may strike back.\\n\"the market is still vulnerable to headlines, particularly with regard to trade and any retaliation,\" said quincy krosby, chief market strategist at prudential financial. we\\'re \"waiting for reaction from the european union and reaction from the chinese in terms retaliatory responses.\"\\nalso on investors\\' minds, krosby added, is the two-day federal open market committee monetary policy meeting next week, with wall street preparing for new federal reserve chair jerome powell to lead bankers in raising rates.\\n\"it\\'s jerome powell\\'s first conference and the market expects a rate hike ... [investors] will also be paying attention to his comments and the press conference,\" krosby said. \"he\\'s extremely fluent in the language of the fed, extremely fluent in the thinking of the fed.\"\\nin political news, president donald trump has reportedly decided to remove national security advisor h.r. mcmaster from the u.s. administration, according to a thursday report by the washington post. the white house has, however, denied that any changes are set to emerge within the national security council.\\nadding to the political drama, cbs news reported on friday that white house chief of staff john kelly, too, could depart the administration as early as today. fears that the chief of staff could be on his way out were kept at bay, however, after the wall street journal reported that trump and kelly settled on a temporary \"truce.\"\\nkelly, rattled by president trump\\'s abrupt firing of secretary of state rex tillerson via twitter earlier in the week, had told colleagues to start looking for new jobs, the journal reported. tillerson\\'s dismissal comes a week after gary cohn resigned as the national economic council\\'s director.\\nbrendan mcdermid | retuers nyse trader on the floor \"i think the market has understood for a while that this is a chaotic white house,\" said michael shaoul, chairman and ceo of marketfield asset management. he noted that stocks have been trading in a close range recently. \"i think it will take more economically driven or corporate-driven news for the market to make up its mind.\"\\nthe commerce department said housing starts declined 7 percent in february, a bigger-than-expected fall. building permits, meanwhile, fell 7.7 percent last month.\\nelsewhere, consumer sentiment rose to a level not seen since 2004 in march, according to a preliminary reading from the university of michigan. meanwhile, the labor department said job openings increased to 6.3 million in january, a record.\\n\"this week investors have been focused on washington,\" said jeff kravetz, regional investment strategist at u.s. bank wealth management. \"but the narrative seems to be changing with strong economic numbers.\"\\n\"i think investors are going to be focused on economic data\" and the federal reserve next week, kravetz said.\\nin corporate news, adobe systems reported better-than-expected quarterly earnings, sending its stock up 3.1 percent.\\nmeanwhile, walmart responded to accusations of issuing misleading e-commerce results, calling the person a \"disgruntled former associate.\" walmart shares rose 1.9 percent.\\n—cnbc\\'s jacob pramuk contributed to this report',\n",
       " 'march 20, 2018 / 1:42 pm / updated 10 minutes ago cambridge analytica ceo claims influence on u.s. election, facebook faces questions eric auchard , david ingram 6 min read london/san francisco (reuters) - the suspended chief executive of cambridge analytica said in a secretly recorded video broadcast on tuesday that his uk-based political consultancy’s online campaign played a decisive role in   trump’s 2016 election victory. his comments, which could not be verified, are potentially a further problem for facebook inc as it faces lawmakers’ scrutiny in the united states and europe over cambridge analytica’s improper use of 50 million facebook users’ personal data. the social media network’s shares fell for a second day, closing down 2.5 percent to $168.15, as investors worried that its dealings with cambridge analytica might damage its reputation, deter advertisers and invite restrictive regulation. the company has lost $60 billion of its stock market value over the last two days. cambridge analytica said on tuesday its board of directors suspended ceo alexander nix, shortly before the second part of british broadcaster channel 4’s expose of the firm’s methods. in the program nix boasts he met trump when he was the republican presidential candidate “many times”. nix’s comments “do not represent the values or operations of the firm and his suspension reflects the seriousness with which we view this violation,” cambridge analytica said in a statement on tuesday. u.s. and european lawmakers have demanded an explanation of how cambridge analytica gained access to user data in 2014 and why facebook failed to inform its users, raising broader industry questions about consumer privacy. facebook said it had been told by the federal trade commission (ftc), the leading u.s. consumer regulator, that it would receive a letter this week with questions about the data acquired by cambridge analytica. it said it had no indication of a formal investigation. “the entire company is outraged we were deceived,” facebook said in a statement on tuesday. “we are committed to vigorously enforcing our policies to protect people’s information and will take whatever steps are required to see that this happens.” ftc review the ftc is reviewing whether facebook violated a 2011 consent decree it reached with the authority over its privacy practices, a person   told reuters. “we are aware of the issues that have been raised but cannot comment on whether we are investigating. we take any allegations of violations of our consent decrees very seriously as we did in 2012 in a privacy case involving google,” an ftc spokesman said. under the 2011 settlement, facebook agreed to get user consent for certain changes to privacy settings as part of a settlement of federal charges that it deceived consumers and forced them to share more personal information than they intended, bloomberg reported. if the ftc finds facebook violated terms of the consent decree, it has the power to fine the company thousands of dollars a day per violation. in a research note, deutsche bank analysts said government scrutiny could hurt facebook’s ability to gather and deploy data for advertising targeting - critical to its growth. people walk past the building housing the offices of cambridge analytica in central london, britain, march 20, 2018. reuters/henry nicholls fear of regulation hurt other social media firms. shares of snap inc fell 2.5 percent and twitter inc fell more than 10 percent. facebook and its peers alphabet inc’s google and twitter already face a backlash over their role during the u.s. presidential election by allowing the spread of false information that might have swayed voters toward trump. u.s. senator dianne feinstein, the top democrat on the judiciary committee, called on tuesday for facebook ceo mark zuckerberg to testify in congress. congressional staff said the company would brief u.s. senate and house aides on wednesday. a congressional official said house intelligence committee democrats plan to interview cambridge analytica whistleblower christopher wylie. the committee interviewed nix by video teleconference, according to the congressional official, but a transcript of that interview has not yet been made public. slideshow (6 images) the senate intelligence committee, which is conducting a long-term investigation of alleged russian interference in u.s. politics and a detailed examination of u.s. election security precautions, would carry out its own inquiry of cambridge analytica, a congressional official with direct knowledge of the investigation said. the white house said it welcomed inquiries, and that the president believes that americans’ privacy should be protected. personal information in britain, the information commissioner’s office, an independent authority set up to uphold information rights in the public interest, was seeking a warrant from a judge to search the offices of london-based cambridge analytica. it was unclear late on tuesday whether it had obtained it. created in 2013, cambridge analytica markets itself as a source of consumer research, targeted advertising and other data-related services to both political and corporate clients. related coverage',\n",
       " 'by jonathan vanian 4:01 am edt \\nmicrosoft plans to open new data center facilities in switzerland and the middle east, specifically abu dhabi and dubai. \\nthe technology giant revealed the new data center projects, part of its azure cloud computing business, on wednesday but did not say when they would be ready to use. \\nthe company also said that a cloud computing data center in france is now open to take business. microsoft first said it would open a france data center in october, 2016. businesses operating in france now have the option to buy on-demand computing resources from microsoft’s azure unit and also subscribe to its office 365 workplace software, delivered from the cloud data center. the company’s dynamics 365 customer relationship management software will be available in early 2019. \\nadditionally, microsoft said it would open new cloud data centers in germany to better serve customers in the country, confirming earlier reports by german news outlets wirtschaftswoche and handelsblatt. microsoft (msft) did not say how much the german data deals cost, but the local news reports said it would cost microsoft the equivalent of $120 million. \\nthe new data centers underscores how cloud computing giants like amazon (amzn) , microsoft, google (goog) , and ibm (ibm) are spending billions of dollars building out their infrastructure across the world so they can sell their technologies to non-u.s. customers. \\noracle (orcl) , which is a making a big cloud computing push, also said in february that it plans to build data centers in countries like china, india, japan, switzerland, and saudi arabia. \\nmany international companies are precluded from storing data outside their countries because of regional data sovereignty laws. as a result, cloud-computing companies are increasingly building facilities in these countries to better serve customers there and abide by local laws. \\n“what our customers appreciate is choice,” said microsoft azure’s head of global infrastructure tom keane in an interview. “there are absolutely some customers that value data residency and it is essential.” \\nthe types of international companies that want to store data within their country while using cloud services tend to be heavily regulated companies like insurance providers, financial services firms, and healthcare companies, he explained. \\nkeane declined to comment on whether microsoft is building entirely new data centers in its newly announced regions or is leasing existing data center facilities to sell its cloud services. \\nget data sheet , fortune’s technology newsletter. \\nit can be cheaper and easier to deal with existing regulatory requirements if cloud-computing companies choose to lease and operate in existing data center facilities while also partnering with local companies. the tradeoff is that they don’t control the entire data centers and can’t optimize them to be as power efficient as the massive data center facilities companies like google, amazon, and microsoft operate in the u.s. \\nkeane also declined to talk about future data center locations, only to say that they would be built in places where microsoft likely sees big business opportunities. that means, data center facilities in far-flung locations like antarctica are likely not coming anytime soon. \\n“if it’s economically viable, i’ll be happy to talk to you,” keane said when asked about possible antarctica or arctic data centers. sponsored financial content',\n",
       " '0 comments good morning! lara is away, so i’m stepping in as temporary captain of the good ship cmo today. our first port of call is amsterdam, where dutch brewer heineken nv is facing a maelstrom of controversy after airing an ad with the tagline “sometimes lighter is better.” the brewer acknowledged that its ad “missed the mark,” saying the tagline intended to evoke its heineken light beer and not, as critics suggested, racial implications. chance the rapper disagreed, calling the ad “terribly racist” on twitter and suggesting the company was deliberately “baiting consumers and tweeters.” watch the ad and decide for yourself.\\nftc pokes facebook the hits keep coming for facebook. after the guardian and the new york times revealed that data company cambridge analytica improperly accessed reams of the social giant’s user data, speculation quickly turned to the possibility of regulators jumping in. on monday, the federal trade commission confirmed it has opened an investigation into the social network, hours after the company acknowledged that it logs the text and call histories of some android users, wsj’s robert mcmillan reports . separately, the attorneys general of 37 states and territories have formally demanded explanations from facebook. meanwhile, facebook chief executive mark zuckerberg has continued something of an apology tour. look no further than the pages of major newspapers, where facebook took out full-page ads signed by mr. zuckerberg personally apologizing for the breach of trust.\\n— regulation interrogation —\\nmr. zuckerberg has chosen his words carefully when responding to questions about government regulation of facebook. in interviews with wired and cnn , the facebook founder endorsed elements of the honest ads act, a bill that would increase transparency around political advertising on the internet. but he has not endorsed any new regulations around privacy or handling of user data, the issues thrown up by the cambridge analytica crisis. what would a regulatory response to the fallout look like? and would facebook executives support it? we might get some answers at a congressional hearing about the future of privacy.\\nguess who’s back...back again what’s old is new at meredith corp. the company is reorganizing ad sales personnel at time inc. , which it recently acquired, to emphasize individual titles rather than industry-specific categories, wsj’s jeffrey trachtenberg reports. the shift effectively reverses a strategy that time inc. had begun putting in place two years ago, under which its ad teams focused on major categories like pharmaceuticals, technology, telecom and automotive. the new strategy is intended to strengthen ties with ad agencies and marketers and give specific magazines greater visibility at a time when print ad revenue continues to be under pressure. the realignment includes reappointing individual publishers at time inc. titles, including those brands meredith has put on the sales block: time, fortune, money and sports illustrated. the goal is to correct what meredith sees as an unforced error by the previous management team.\\napple pays publishers have long complained that apple doesn’t allow them to make money from the readership they get through apple news. that equation could be changing, according to digiday’s tim peterson , now that apple is increasing the amount of publishers that can serve ads on apple news using doubleclick for publishers. as publishers explore alternatives to facebook, they’re finding representatives from apple news greeting them with open arms. despite the olive branch, some publishers may never fully embrace apple news because the app does not send readers to their websites, essentially serving as an intermediary at a time when news organizations are seeking to build direct relationships with their audiences.\\nno deal? no problem! that’s the consensus from sources at cnn who spoke with vanity fair’s joe pompeo for a look at the network’s prospects if time warner’s merger with at&t doesn’t survive a legal challenge from antitrust enforcers. time warner has been a safe harbor for cnn, according to people close to cnn worldwide president jeff zucker, who suggested that employees are more concerned with the prospect of life under at&t than they are at a traditional content company. their jobs might be safer in a combined company with the scale and distribution to compete with the likes of google and facebook — but then again, it’s possible that the platforms might be in the antitrust crosshairs before long.\\nbest of the rest google’s approach to europe’s general data protection regulation could be a template for other tech giants. [ digiday ]\\nad-buying agency groupm is requiring publishers to sign a data-protection contract. [ digiday ]\\nthe advertising research foundation is preparing to release data privacy guidelines. [ axios ]\\nespn is betting that its forthcoming streaming service, espn+, will help offset declines from cord-cutters, but it must contend with serious competition from rivals and has an untested new executive. [ variety ]\\ndigital media holding company iac is selling dictionary.com and its sister site, thesaurus.com. [ wsj ]\\nthe “60 minutes” interview with adult film actress stormy daniels delivered the show’s best ratings in the past decade. [ wsj ]\\nnickelodeon has cut ties with prolific producer dan schneider. [ deadline ]\\nvice hired former havas executive dominique delport as its global chief revenue officer and president of international operations. [ adweek ]\\nabout us follow us on twitter: @wsjcmo , @larakiara , @vranicawsj , @alexbruell , @benmullin , @srabil , @asharma\\nsubscribe to our morning newsletter, delivered straight to your inbox, at http://on.wsj.com/cmotodaysignup .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.0317640292906467,\n",
       " -0.7433071978199797,\n",
       " 0.40603849693958644,\n",
       " -1.33516004603624,\n",
       " -2.6165929254508913]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_news_to_words(news):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    news = re.sub(r\"[^a-zA-Z0-9]\", \" \", news.lower())\n",
    "    words = news.split()\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'news',\n",
       " 'texa',\n",
       " 'serial',\n",
       " 'bomber',\n",
       " 'made',\n",
       " 'video',\n",
       " 'confess',\n",
       " 'blow',\n",
       " 'polic',\n",
       " 'serial',\n",
       " 'bomber',\n",
       " 'whose',\n",
       " 'deadli',\n",
       " 'attack',\n",
       " 'terror',\n",
       " 'austin',\n",
       " 'texa',\n",
       " 'week',\n",
       " 'left',\n",
       " '25',\n",
       " 'minut',\n",
       " 'video',\n",
       " 'confess',\n",
       " 'cell',\n",
       " 'phone',\n",
       " 'suspect',\n",
       " 'blew',\n",
       " 'wednesday',\n",
       " 'offic',\n",
       " 'close',\n",
       " 'make',\n",
       " 'arrest',\n",
       " 'video',\n",
       " 'fail',\n",
       " 'reveal',\n",
       " 'coher',\n",
       " 'motiv',\n",
       " 'attack',\n",
       " 'publish',\n",
       " '6',\n",
       " 'hour',\n",
       " 'ago',\n",
       " 'reuter',\n",
       " 'sourc',\n",
       " 'sinclair',\n",
       " 'broadcast',\n",
       " 'group',\n",
       " 'mark',\n",
       " 'anthoni',\n",
       " 'conditt',\n",
       " 'suspect',\n",
       " 'austin',\n",
       " 'bomb',\n",
       " 'scene',\n",
       " 'secur',\n",
       " 'video',\n",
       " 'fedex',\n",
       " 'facil',\n",
       " 'serial',\n",
       " 'bomber',\n",
       " 'whose',\n",
       " 'deadli',\n",
       " 'attack',\n",
       " 'terror',\n",
       " 'austin',\n",
       " 'texa',\n",
       " 'week',\n",
       " 'left',\n",
       " '25',\n",
       " 'minut',\n",
       " 'video',\n",
       " 'confess',\n",
       " 'cell',\n",
       " 'phone',\n",
       " 'found',\n",
       " 'blew',\n",
       " 'wednesday',\n",
       " 'offic',\n",
       " 'close',\n",
       " 'make',\n",
       " 'arrest',\n",
       " 'polic',\n",
       " 'said',\n",
       " 'mark',\n",
       " 'conditt',\n",
       " '23',\n",
       " 'unemploy',\n",
       " 'man',\n",
       " 'suburb',\n",
       " 'pflugervil',\n",
       " 'detail',\n",
       " 'made',\n",
       " 'seven',\n",
       " 'bomb',\n",
       " 'account',\n",
       " 'five',\n",
       " 'explod',\n",
       " 'one',\n",
       " 'recov',\n",
       " 'went',\n",
       " 'seventh',\n",
       " 'deton',\n",
       " 'offic',\n",
       " 'rush',\n",
       " 'vehicl',\n",
       " 'earli',\n",
       " 'wednesday',\n",
       " 'video',\n",
       " 'fail',\n",
       " 'reveal',\n",
       " 'coher',\n",
       " 'motiv',\n",
       " 'attack',\n",
       " 'spread',\n",
       " 'past',\n",
       " 'three',\n",
       " 'week',\n",
       " 'polic',\n",
       " 'said',\n",
       " 'mention',\n",
       " 'anyth',\n",
       " 'terror',\n",
       " 'mention',\n",
       " 'anyth',\n",
       " 'hate',\n",
       " 'instead',\n",
       " 'outcri',\n",
       " 'challeng',\n",
       " 'young',\n",
       " 'man',\n",
       " 'talk',\n",
       " 'challeng',\n",
       " 'person',\n",
       " 'life',\n",
       " 'austin',\n",
       " 'polic',\n",
       " 'chief',\n",
       " 'brian',\n",
       " 'manley',\n",
       " 'told',\n",
       " 'report',\n",
       " 'would',\n",
       " 'classifi',\n",
       " 'confess',\n",
       " 'manley',\n",
       " 'said',\n",
       " 'conditt',\n",
       " 'never',\n",
       " 'troubl',\n",
       " 'law',\n",
       " 'kill',\n",
       " 'two',\n",
       " 'peopl',\n",
       " 'wound',\n",
       " 'five',\n",
       " 'campaign',\n",
       " 'violenc',\n",
       " 'began',\n",
       " 'march',\n",
       " '2',\n",
       " 'author',\n",
       " 'said',\n",
       " 'base',\n",
       " 'search',\n",
       " 'suspect',\n",
       " 'home',\n",
       " 'video',\n",
       " 'statement',\n",
       " 'author',\n",
       " 'said',\n",
       " 'felt',\n",
       " 'confid',\n",
       " 'bomb',\n",
       " 'public',\n",
       " 'safe',\n",
       " 'harm',\n",
       " 'fbi',\n",
       " 'special',\n",
       " 'agent',\n",
       " 'christoph',\n",
       " 'comb',\n",
       " 'said',\n",
       " 'investig',\n",
       " 'believ',\n",
       " 'suspect',\n",
       " 'would',\n",
       " 'continu',\n",
       " 'attack',\n",
       " 'apprehend',\n",
       " 'polic',\n",
       " 'recov',\n",
       " 'target',\n",
       " 'list',\n",
       " 'address',\n",
       " 'futur',\n",
       " 'bomb',\n",
       " 'lo',\n",
       " 'angel',\n",
       " 'time',\n",
       " 'report',\n",
       " 'cite',\n",
       " 'u',\n",
       " 'repres',\n",
       " 'michael',\n",
       " 'mccaul',\n",
       " 'texa',\n",
       " 'republican',\n",
       " 'chairman',\n",
       " 'hous',\n",
       " 'homeland',\n",
       " 'secur',\n",
       " 'committe',\n",
       " 'even',\n",
       " 'video',\n",
       " 'gave',\n",
       " 'explan',\n",
       " 'individu',\n",
       " 'address',\n",
       " 'singl',\n",
       " 'recipi',\n",
       " 'bomb',\n",
       " 'plant',\n",
       " 'ship',\n",
       " 'manley',\n",
       " 'said',\n",
       " 'polic',\n",
       " 'previous',\n",
       " 'said',\n",
       " 'consid',\n",
       " 'possibl',\n",
       " 'attack',\n",
       " 'racial',\n",
       " 'motiv',\n",
       " 'note',\n",
       " 'first',\n",
       " 'sever',\n",
       " 'victim',\n",
       " 'includ',\n",
       " 'two',\n",
       " 'die',\n",
       " 'either',\n",
       " 'african',\n",
       " 'american',\n",
       " 'hispan',\n",
       " 'conditt',\n",
       " 'like',\n",
       " 'record',\n",
       " 'video',\n",
       " '9',\n",
       " 'p',\n",
       " '11',\n",
       " 'p',\n",
       " 'tuesday',\n",
       " 'accord',\n",
       " 'manley',\n",
       " 'conditt',\n",
       " 'said',\n",
       " 'believ',\n",
       " 'polic',\n",
       " 'get',\n",
       " 'close',\n",
       " 'right',\n",
       " 'author',\n",
       " 'file',\n",
       " 'crimin',\n",
       " 'complaint',\n",
       " 'issu',\n",
       " 'arrest',\n",
       " 'warrant',\n",
       " 'around',\n",
       " 'time',\n",
       " 'wednesday',\n",
       " 'morn',\n",
       " 'polic',\n",
       " 'track',\n",
       " 'conditt',\n",
       " 'hotel',\n",
       " 'wait',\n",
       " 'arriv',\n",
       " 'tactic',\n",
       " 'unit',\n",
       " 'equip',\n",
       " 'plan',\n",
       " 'make',\n",
       " 'arrest',\n",
       " 'manley',\n",
       " 'said',\n",
       " 'conditt',\n",
       " 'drove',\n",
       " 'away',\n",
       " 'polic',\n",
       " 'follow',\n",
       " 'decid',\n",
       " 'stop',\n",
       " 'got',\n",
       " 'highway',\n",
       " 'offic',\n",
       " 'approach',\n",
       " 'vehicl',\n",
       " 'explos',\n",
       " 'went',\n",
       " 'manley',\n",
       " 'said',\n",
       " 'also',\n",
       " 'polic',\n",
       " 'shoot',\n",
       " 'never',\n",
       " 'call',\n",
       " 'happi',\n",
       " 'end',\n",
       " 'damn',\n",
       " 'good',\n",
       " 'one',\n",
       " 'peopl',\n",
       " 'commun',\n",
       " 'peopl',\n",
       " 'state',\n",
       " 'texa',\n",
       " 'travi',\n",
       " 'counti',\n",
       " 'district',\n",
       " 'attorney',\n",
       " 'margaret',\n",
       " 'moor',\n",
       " 'told',\n",
       " 'report',\n",
       " 'resid',\n",
       " 'austin',\n",
       " 'citi',\n",
       " '1',\n",
       " 'million',\n",
       " 'peopl',\n",
       " 'liber',\n",
       " 'enclav',\n",
       " 'univers',\n",
       " 'student',\n",
       " 'tech',\n",
       " 'compani',\n",
       " 'voic',\n",
       " 'relief',\n",
       " 'hunt',\n",
       " 'serial',\n",
       " 'bomber',\n",
       " 'go',\n",
       " 'leeri',\n",
       " 'extra',\n",
       " 'care',\n",
       " 'tomorrow',\n",
       " 'work',\n",
       " 'feel',\n",
       " 'reliev',\n",
       " 'said',\n",
       " 'jesu',\n",
       " 'borjon',\n",
       " '44',\n",
       " 'employe',\n",
       " 'parcel',\n",
       " 'deliveri',\n",
       " 'firm',\n",
       " 'up',\n",
       " 'live',\n",
       " 'pflugervil',\n",
       " 'austin',\n",
       " 'host',\n",
       " 'thousand',\n",
       " 'town',\n",
       " 'visitor',\n",
       " 'annual',\n",
       " 'south',\n",
       " 'southwest',\n",
       " 'festiv',\n",
       " 'music',\n",
       " 'film',\n",
       " 'technolog',\n",
       " 'first',\n",
       " 'bomb',\n",
       " 'occur',\n",
       " 'trail',\n",
       " 'clue',\n",
       " 'trail',\n",
       " 'clue',\n",
       " 'lead',\n",
       " 'hundr',\n",
       " 'investig',\n",
       " 'serial',\n",
       " 'bomber',\n",
       " 'rang',\n",
       " 'store',\n",
       " 'receipt',\n",
       " 'fragment',\n",
       " 'boobi',\n",
       " 'trap',\n",
       " 'packag',\n",
       " 'surveil',\n",
       " 'video',\n",
       " 'suspect',\n",
       " 'hat',\n",
       " 'wig',\n",
       " 'expert',\n",
       " 'scour',\n",
       " 'suspect',\n",
       " 'home',\n",
       " 'evid',\n",
       " 'wednesday',\n",
       " 'remov',\n",
       " 'explos',\n",
       " 'materi',\n",
       " 'bomb',\n",
       " 'compon',\n",
       " 'call',\n",
       " 'bomb',\n",
       " 'make',\n",
       " 'factori',\n",
       " 'definit',\n",
       " 'compon',\n",
       " 'consist',\n",
       " 'seen',\n",
       " 'devic',\n",
       " 'fred',\n",
       " 'milanowski',\n",
       " 'special',\n",
       " 'agent',\n",
       " 'charg',\n",
       " 'houston',\n",
       " 'offic',\n",
       " 'bureau',\n",
       " 'alcohol',\n",
       " 'tobacco',\n",
       " 'firearm',\n",
       " 'explos',\n",
       " 'told',\n",
       " 'report',\n",
       " 'investig',\n",
       " 'evacu',\n",
       " 'four',\n",
       " 'block',\n",
       " 'radiu',\n",
       " 'around',\n",
       " 'conditt',\n",
       " 'hous',\n",
       " 'search',\n",
       " 'home',\n",
       " 'conditt',\n",
       " 'share',\n",
       " 'two',\n",
       " 'roommat',\n",
       " 'detain',\n",
       " 'question',\n",
       " 'conditt',\n",
       " 'move',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'leav',\n",
       " 'parent',\n",
       " 'home',\n",
       " 'mile',\n",
       " '1',\n",
       " '6',\n",
       " 'km',\n",
       " 'away',\n",
       " 'public',\n",
       " 'record',\n",
       " 'show',\n",
       " 'one',\n",
       " 'law',\n",
       " 'enforc',\n",
       " 'offici',\n",
       " 'involv',\n",
       " 'investig',\n",
       " 'speak',\n",
       " 'condit',\n",
       " 'anonym',\n",
       " 'told',\n",
       " 'reuter',\n",
       " 'materi',\n",
       " 'found',\n",
       " 'remnant',\n",
       " 'bomb',\n",
       " 'trace',\n",
       " 'back',\n",
       " 'sold',\n",
       " 'sourc',\n",
       " 'also',\n",
       " 'said',\n",
       " 'investig',\n",
       " 'identifi',\n",
       " 'conditt',\n",
       " 'potenti',\n",
       " 'suspect',\n",
       " 'obtain',\n",
       " 'warrant',\n",
       " 'monitor',\n",
       " 'googl',\n",
       " 'search',\n",
       " 'histori',\n",
       " 'surveil',\n",
       " 'video',\n",
       " 'show',\n",
       " 'suspect',\n",
       " 'hat',\n",
       " 'blond',\n",
       " 'wig',\n",
       " 'prepar',\n",
       " 'ship',\n",
       " 'one',\n",
       " 'two',\n",
       " 'boobi',\n",
       " 'trap',\n",
       " 'packag',\n",
       " 'known',\n",
       " 'sent',\n",
       " 'fedex',\n",
       " 'corp',\n",
       " 'deliveri',\n",
       " 'servic',\n",
       " 'accord',\n",
       " 'sourc',\n",
       " 'use',\n",
       " 'alia',\n",
       " 'kelli',\n",
       " 'killmor',\n",
       " 'ship',\n",
       " 'packag',\n",
       " 'abc',\n",
       " 'news',\n",
       " 'report',\n",
       " 'cite',\n",
       " 'unnam',\n",
       " 'law',\n",
       " 'enforc',\n",
       " 'sourc',\n",
       " 'conditt',\n",
       " 'home',\n",
       " 'school',\n",
       " 'describ',\n",
       " 'conserv',\n",
       " 'said',\n",
       " 'polit',\n",
       " 'inclin',\n",
       " 'accord',\n",
       " 'blog',\n",
       " 'post',\n",
       " 'wrote',\n",
       " 'part',\n",
       " 'u',\n",
       " 'polit',\n",
       " 'class',\n",
       " 'austin',\n",
       " 'commun',\n",
       " 'colleg',\n",
       " 'attend',\n",
       " '2010',\n",
       " '2012',\n",
       " 'record',\n",
       " 'disciplinari',\n",
       " 'action',\n",
       " 'school',\n",
       " 'said']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_news_to_words(google_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cache_dir = '../cache'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def preprocess_data(X, y, cache_file, train_size=0.8):\n",
    "    \n",
    "    cache_data = None\n",
    "    try:\n",
    "        with open(os.path.join(cache_dir, cache_file), 'rb') as f:\n",
    "            cache_data = pickle.load(f)\n",
    "        print(\"Cache data read from {}\".format(cache_file))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if cache_data is None:\n",
    "        words = [convert_news_to_words(news) for news in X]\n",
    "        \n",
    "        train_valid_words, test_words, train_valid_y, test_y = train_test_split(words, y, train_size=train_size)\n",
    "        train_words, valid_words, train_y, valid_y = train_test_split(train_valid_words, train_valid_y, train_size=train_size)\n",
    "        cache_data = dict(train_words=train_words, test_words=test_words, valid_words=valid_words, train_y=train_y, test_y=test_y, valid_y=valid_y)\n",
    "        with open(os.path.join(cache_dir, cache_file), 'wb') as f:\n",
    "            pickle.dump(cache_data, f)\n",
    "    else:\n",
    "        train_words, test_words, valid_words, train_y, test_y, valid_y = (cache_data['train_words'], cache_data['test_words'],\n",
    "            cache_data['valid_words'], cache_data['train_y'], cache_data['test_y'], cache_data['valid_y'])\n",
    "    \n",
    "    return train_words, test_words, valid_words, train_y, test_y, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "google_train_words, google_test_words, google_valid_words, google_train_y, google_test_y, google_valid_y = preprocess_data(google_X, google_y, 'google_cache.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "amazon_train_words, amazon_test_words, amazon_valid_words, amazon_train_y, amazon_test_y, amazon_valid_y = preprocess_data(amazon_X, amazon_y, 'amazon_cache.pickle')\n",
    "fb_train_words, fb_test_words, fb_valid_words, fb_train_y, fb_test_y, fb_valid_y = preprocess_data(facebook_X, facebook_y, 'fb_cache.pickle')\n",
    "msft_train_words, msft_test_words, msft_valid_words, msft_train_y, msft_test_y, msft_valid_y = preprocess_data(microsoft_X, microsoft_y, 'msft_cache.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.05382035347906716,\n",
       " 0.8707537156320021,\n",
       " -1.0610991005164927,\n",
       " 0.4273119966757732,\n",
       " -0.6338152112165558,\n",
       " -1.1118349408332278,\n",
       " 0.6046072976905025,\n",
       " 0.1488370332861797,\n",
       " 0.6573143846286322,\n",
       " 0.7724915003146318]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_train_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(words, file_name, vocab_size=5000):\n",
    "    \n",
    "    word_count = {}\n",
    "    for news in words:\n",
    "        for word in news:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    \n",
    "    sorted_words = sorted(word_count.keys(), key=lambda x: word_count[x], reverse=True)\n",
    "    \n",
    "    word_dict = {}\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]):\n",
    "        word_dict[word] = idx + 2 \n",
    "        \n",
    "    with open(os.path.join(data_dir, file_name), \"wb\") as f:\n",
    "        pickle.dump(word_dict, f)\n",
    "    \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_dict = build_dict(google_train_words, \"google_dict.pickle\")\n",
    "amazon_dict = build_dict(amazon_train_words, \"amazon_dict.pickle\")\n",
    "fb_dict = build_dict(fb_train_words, \"fb_dict.pickle\")\n",
    "msft_dict = build_dict(msft_train_words, \"msft_dict.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_pad(word_dict, sentence, pad_length=500):\n",
    "    NOWORD = 0\n",
    "    INFREQ = 1\n",
    "    \n",
    "    vectorized_sentence = [NOWORD] * pad_length\n",
    "    \n",
    "    for index, word in enumerate(sentence[:pad_length]):\n",
    "        if word in word_dict:\n",
    "            vectorized_sentence[index] = word_dict[word]\n",
    "        else:\n",
    "            vectorized_sentence[index] = INFREQ\n",
    "    \n",
    "    return vectorized_sentence, min(len(sentence), pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_pad_sentences(sentences, word_dict):\n",
    "    \n",
    "    vectorized_sentences = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        vectorized_sentence, length = vectorize_and_pad(word_dict, sentence)\n",
    "        vectorized_sentences.append(vectorized_sentence)\n",
    "        lengths.append(length)\n",
    "    \n",
    "    return np.array(vectorized_sentences), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_vectorized_sentences, google_lengths = vectorize_and_pad_sentences(google_train_words, google_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 796,  123,  255, 1746,  492, 1149,   70,  368,  915,  158, 1105,\n",
       "        162,  729,  124,   49, 2139, 3563, 2004,  255, 1746,  796,  136,\n",
       "       2782, 4598,  218,   20, 1789,  953,    5,  668,  255, 2748, 1651,\n",
       "       4087,    1, 1944,  552,  863,    1,  411,  267, 1450, 1283, 3031,\n",
       "        261,  863, 1944, 1352,  915,  574, 2380, 1105, 4598,    3,  566,\n",
       "        796,  208, 4404,  162,  555, 1105, 1944, 1176,  329,  814,  162,\n",
       "          1,    1, 2783,  154,  249,  138,  154,   27, 1944,  162,  570,\n",
       "        182,  869,  198, 1145,  751, 1660, 1105,  503, 3280,  486,  112,\n",
       "        153, 4598,    3, 4598,    3, 1944,  560, 1208,  612,  313, 1567,\n",
       "        162, 1894,  356,  172,  400, 1208,  612, 1944,  671,  468, 1726,\n",
       "          1,    1,    1,    1,   16, 1105, 4598,  208,  427, 4808,    3,\n",
       "        255, 1746,  796,  568, 1944,  480,  908,  261, 2711,  162, 1894,\n",
       "       1105, 1925,   62,  283,  242, 2711, 3417, 1237,    1,  862,  342,\n",
       "         20,  779, 1944,    1,  269, 3635,  555, 1105, 4598,    3,  552,\n",
       "        319,   62, 1091,  275,  122,  255, 2808,    1,   20, 1944, 2861,\n",
       "          1, 1944,  761,  431,   20,  882,  796, 2235,  282,  796,  123,\n",
       "        274, 1944, 1984,  269,  370,  192, 1661,   20,  123,  196,  738,\n",
       "       1190,  836, 3366,  501, 1462,  882,  188, 2862,  492, 2235,  282,\n",
       "        796, 1985,    1, 4239,  218, 1661, 4598,    3,  255, 1746,  796,\n",
       "        738, 2235,  282,  796,  555, 1105,   86,   15,  806,  475,   32,\n",
       "        984,  915,  124,   49, 2004, 1105, 4598,    3,  208,  301,  162,\n",
       "        197,  255,  399,  125,  930,  606,  218,  255,  399, 4162,  368,\n",
       "        172,  110, 1260, 1368,  238,  192,  359, 1368,  717, 1745,  580,\n",
       "       1527,  215,  368,   31,  921,  533,    1,  162,  219,  159,   97,\n",
       "       1105,    3,   88,   75,  123,  893,   42,   75,  940,  165,  209,\n",
       "        165,  215,  162,    1,  229,   47,    1,    6,  253,    6,    1,\n",
       "        284,   31,   17,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_vectorized_sentences[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_vectorized_sentences, amazon_lengths = vectorize_and_pad_sentences(amazon_train_words, amazon_dict)\n",
    "fb_vectorized_sentences, fb_lengths = vectorize_and_pad_sentences(fb_train_words, fb_dict)\n",
    "msft_vectorized_sentences, msft_lengths = vectorize_and_pad_sentences(msft_train_words, msft_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.concat([pd.DataFrame(google_train_y), pd.DataFrame(google_lengths), pd.DataFrame(google_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'google_train.csv'), header=False, index=False)\n",
    "pd.concat([pd.DataFrame(amazon_train_y), pd.DataFrame(amazon_lengths), pd.DataFrame(amazon_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'amazon_train.csv'), header=False, index=False)\n",
    "pd.concat([pd.DataFrame(fb_train_y), pd.DataFrame(fb_lengths), pd.DataFrame(fb_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'fb_train.csv'), header=False, index=False)\n",
    "pd.concat([pd.DataFrame(msft_train_y), pd.DataFrame(msft_lengths), pd.DataFrame(msft_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'msft_train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process and Save Test Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_test_vectorized_sentences, google_test_lengths = vectorize_and_pad_sentences(google_test_words, google_dict)\n",
    "amazon_test_vectorized_sentences, amazon_test_lengths = vectorize_and_pad_sentences(amazon_test_words, amazon_dict)\n",
    "fb_test_vectorized_sentences, fb_test_lengths = vectorize_and_pad_sentences(fb_test_words, fb_dict)\n",
    "msft_test_vectorized_sentences, msft_test_lengths = vectorize_and_pad_sentences(msft_test_words, msft_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '../test_data'\n",
    "os.makedirs(test_data_dir, exist_ok=True)\n",
    "\n",
    "pd.concat([pd.DataFrame(google_test_lengths), pd.DataFrame(google_test_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(test_data_dir, 'google_test.csv'), index=False)\n",
    "pd.concat([pd.DataFrame(amazon_test_lengths), pd.DataFrame(amazon_test_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(test_data_dir, 'amazon_test.csv'), index=False)\n",
    "pd.concat([pd.DataFrame(fb_test_lengths), pd.DataFrame(fb_test_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(test_data_dir, 'fb_test.csv'), index=False)\n",
    "pd.concat([pd.DataFrame(msft_test_lengths), pd.DataFrame(msft_test_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(test_data_dir, 'msft_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(google_test_y).to_csv(os.path.join(test_data_dir, 'google_test_y.csv'), index=False)\n",
    "pd.DataFrame(amazon_test_y).to_csv(os.path.join(test_data_dir, 'amazon_test_y.csv'), index=False)\n",
    "pd.DataFrame(fb_test_y).to_csv(os.path.join(test_data_dir, 'fb_test_y.csv'), index=False)\n",
    "pd.DataFrame(msft_test_y).to_csv(os.path.join(test_data_dir, 'msft_test_y.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process and Save Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_valid_vectorized_sentences, google_valid_lengths = vectorize_and_pad_sentences(google_valid_words, google_dict)\n",
    "amazon_valid_vectorized_sentences, amazon_valid_lengths = vectorize_and_pad_sentences(amazon_valid_words, amazon_dict)\n",
    "fb_valid_vectorized_sentences, fb_valid_lengths = vectorize_and_pad_sentences(fb_valid_words, fb_dict)\n",
    "msft_valid_vectorized_sentences, msft_valid_lengths = vectorize_and_pad_sentences(msft_valid_words, msft_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(google_valid_y), pd.DataFrame(google_valid_lengths), pd.DataFrame(google_valid_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'google_valid.csv'), header=False, index=False)\n",
    "pd.concat([pd.DataFrame(amazon_valid_y), pd.DataFrame(amazon_valid_lengths), pd.DataFrame(amazon_valid_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'amazon_valid.csv'), header=False, index=False)\n",
    "pd.concat([pd.DataFrame(fb_valid_y), pd.DataFrame(fb_valid_lengths), pd.DataFrame(fb_valid_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'fb_valid.csv'), header=False, index=False)\n",
    "pd.concat([pd.DataFrame(msft_valid_y), pd.DataFrame(msft_valid_lengths), pd.DataFrame(msft_valid_vectorized_sentences)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'msft_valid.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
